<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-12-20T15:43:42+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Daniel Worrall</title><subtitle>Machine Learning Researcher</subtitle><author><name>Daniel Worrall</name><email>dworrall at qti.qualcomm.com</email></author><entry><title type="html">Reversible optimizers</title><link href="http://localhost:4000/blog/2020/12/reversible-optimizers/" rel="alternate" type="text/html" title="Reversible optimizers" /><published>2020-12-20T00:00:00+01:00</published><updated>2020-12-20T00:00:00+01:00</updated><id>http://localhost:4000/blog/2020/12/reversible-optimizers</id><content type="html" xml:base="http://localhost:4000/blog/2020/12/reversible-optimizers/">&lt;p&gt;I&lt;/p&gt;</content><author><name>Daniel Worrall</name><email>dworrall at qti.qualcomm.com</email></author><category term="optimization" /><category term="reversibility" /><summary type="html">Reversible neural architectures have been a popular research area in the last few years, but reversibility is far from new in the optimization world.</summary></entry><entry><title type="html">Effective theories</title><link href="http://localhost:4000/randomness2/" rel="alternate" type="text/html" title="Effective theories" /><published>2019-12-29T01:00:00+01:00</published><updated>2019-12-29T01:00:00+01:00</updated><id>http://localhost:4000/randomness2</id><content type="html" xml:base="http://localhost:4000/randomness2/">&lt;p&gt;&lt;strong&gt;This is a follow-on post from &lt;a href=&quot;/blog/2019/12/the-invention-of-randomness/&quot;&gt;“On the ‘invention’ of randomness”&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&quot;get-real&quot;&gt;Get real&lt;/h1&gt;
&lt;p&gt;In my &lt;a href=&quot;/blog/2019/12/the-invention-of-randomness/&quot;&gt;previous post&lt;/a&gt; I talked about the &lt;em&gt;invention of randomness&lt;/em&gt;. Randomness, some claim, can be viewed as a post hoc invention used to fill in the cracks and account for observer ignorance. Critically, it is a quantitative way to model everything I do not know and everything I do know but am too lazy to model. You might say it is the theory of known unknowns. This closed on a (somewhat uninformed) discussion on quantum theory, where I pointed towards the conundrum in the quantum physics community as to how to interpret wavefunctions. What is striking is that quantum mechanics works really well, the maths appears to be correct as far as we all know. The issue is that we just don’t know what it describes.&lt;/p&gt;

&lt;p&gt;This thought left me with a slight feeling of déjà vu. It smacks a bit of an &lt;em&gt;effective theory&lt;/em&gt;. An effective theory is a just fancy name for any theory, which describes &lt;em&gt;effects&lt;/em&gt; and not underlying causes. A much cited example of an effective theory is &lt;em&gt;thermodynamics&lt;/em&gt;. In thermodynamics, we might ask questions such as “If I heat up this gas, how much will it expand”? Such a question is of central importance to the automobile industry. The thermodynamicist’s answer might say something like, if the gas is &lt;em&gt;ideal&lt;/em&gt; and at constant pressure then by Boyle’s Law volume is proportional to temperature et cetera. In the laws of thermodynamics we never actually see what is really going on mechanically when I add heat to my gas. Heating a gas is essentially the act of adding energy to molecules, i.e. we knock them about a bit and make them whizz about faster. This added momentum causes molecules to knock into each other more often and with higher impulse upon contact, causing a rise in pressure, which would lead to a volumetric expansion. Nowhere in all the equations I was tortured with in first-year undergraduate thermodynamics class did we ever talk about molecules. In applying the laws of thermodynamics, engineers regularly skip the fiddly bit of having to model the underlying molecular dynamics giving rise to all the observed phenomena of temperature and pressure etc. That’s because, thermodynamics is an effective theory.&lt;/p&gt;

&lt;p&gt;In fact, in the 19th and early 20th century, many of the laws of thermodynamics had already been recorded empirically. Where they came from, however, was a big mystery. Why should temperature and volume be proportional to one another anyway? The theory that matter was made of atoms inspired the &lt;em&gt;kinetic theory of gases&lt;/em&gt; and the closely linked &lt;em&gt;statistical mechanics&lt;/em&gt;, which sought to address this gap in our understanding. Under the law of large number, the motion of particles explains intrinsic properties such as temperature and pressure. So engineers and lazy people like myself can get away with using a theory that has no connection with the underlying physics of things, but just models observed effects. I find this quite profound. We need not actually complicate ours lives with the cumbersome task of modelling reality. To quote a friend who recently went through a difficult breakup, “Reality sucks.”&lt;/p&gt;

&lt;p&gt;For the purists, however, modelling reality is important. Some, like Judea Pearl, poo poo effective theories as a sophisticated form of curve fitting. Underlying causational models are important in the end, thermodynamics does have its limits: low pressures, high pressures, relativistic velocities and so forth. It is in these limits that we have to resort to the more detailed (and realistic) kinetic theory. One small observation is that we refer to quantities such as pressure, volume, and temperature as &lt;em&gt;properties&lt;/em&gt;. They are collective properties of a gas, since note that we cannot talk of an individual molecule having temperature or pressure or volume. In some sense, these quantities are not fundamental at all, but humans have invented them to make life easier. Why does water freeze? Because it’s cold. In fact, even talking about a liquid freezing only makes sense when we have invented material &lt;em&gt;phases&lt;/em&gt; and begin to think of a collection of water molecules as forming crystals. In analogy to machine learning, we might see these quantities as good &lt;em&gt;features&lt;/em&gt;. Feature themselves are not fundamental, they describe certain configurations of inputs, which are helpful for downstream tasks.&lt;/p&gt;

&lt;h1 id=&quot;where-am-i-heading-with-all-of-this&quot;&gt;Where am I heading with all of this?&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Effective theories: do we need reality? Is reality even important&lt;/li&gt;
  &lt;li&gt;Correlation versus causation&lt;/li&gt;
  &lt;li&gt;The Scientific Method&lt;/li&gt;
  &lt;li&gt;Thermal physics, is temperature real?&lt;/li&gt;
  &lt;li&gt;Gerard ‘t Hooft: automata&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Daniel Worrall</name><email>dworrall at qti.qualcomm.com</email></author><category term="probability" /><category term="bayesianism" /><category term="Jaynes" /><category term="philosophy" /><category term="effective-theories" /><summary type="html">This is a follow-on post from “On the ‘invention’ of randomness”</summary></entry><entry><title type="html">Aleatoric and epistemic uncertainties: a deceptively simple distinction?</title><link href="http://localhost:4000/randomness3/" rel="alternate" type="text/html" title="Aleatoric and epistemic uncertainties: a deceptively simple distinction?" /><published>2019-12-29T01:00:00+01:00</published><updated>2019-12-29T01:00:00+01:00</updated><id>http://localhost:4000/randomness3</id><content type="html" xml:base="http://localhost:4000/randomness3/">&lt;p&gt;*This is a follow-on post from &lt;a href=&quot;/blog/2019/12/the-invention-of-randomness/&quot;&gt;“On the ‘invention’ of randomness”&lt;/a&gt;**&lt;/p&gt;

&lt;h1 id=&quot;noise-is-everything-you-do-not-know&quot;&gt;Noise is everything you do not know&lt;/h1&gt;
&lt;p&gt;Rich Turner.&lt;/p&gt;

&lt;p&gt;Previously, I wrote about the apparent invention of intrinsic randomness; this idea that randomness (colloquially referred to as noise) can only ever be perceived and that it fundamentally may not exist. What does exist is &lt;em&gt;observer ignorance&lt;/em&gt; and sometimes we make the very human mistake of believing that our ignorance about the outside world must in fact be ‘real’, what Jaynes called a &lt;em&gt;mind projection fallacy&lt;/em&gt;. At this point, this is a very cerebral, dare I say, academic distinction. It would be nice to see whether this distinction has any real downstream consequences for the likes of real-world scientists other than particle physicists.&lt;/p&gt;

&lt;p&gt;A problem of much practical significance—and one that I have devoted some time to—is the distinction between &lt;em&gt;aleatoric&lt;/em&gt; and &lt;em&gt;epistemic&lt;/em&gt; uncertainties. This distinction has been acknowledged for quite some time but only received a lot of attention within the deep learning community a few years ago with the release of the paper &lt;a href=&quot;https://arxiv.org/abs/1703.04977&quot;&gt;What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?&lt;/a&gt; (Kendall and Gal, 2017). At pretty much exactly the same time, &lt;a href=&quot;https://rt416.github.io/&quot;&gt;Ryutaro Tanno&lt;/a&gt; and I had released a version of this for the medical imaging community &lt;a href=&quot;https://arxiv.org/abs/1705.00664&quot;&gt;Bayesian Image Quality Transfer with CNNs: Exploring Uncertainty in dMRI Super-Resolution&lt;/a&gt;. Both papers were flawed, this is why.&lt;/p&gt;

&lt;h1 id=&quot;what-are-aleatoric-and-epistemic-uncertainties&quot;&gt;What are aleatoric and epistemic uncertainties&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;Aleatoric&lt;/em&gt; uncertainty describes the statistical aberrations in the consistency of observations. Say you run an experiment \(E\) multiple times with the same settings and each time it returns different observations \(X_1, X_2, ...\), that statistical dance about the mean can be regarded as the aleatoric &lt;em&gt;noise&lt;/em&gt;. If you have real-valued observations, then as simple proxy for aleatoric uncertainty might be the observation variance. No matter how many times you run the experiment, this noise is always present. As such, it is sometimes referred to as &lt;em&gt;irreducible uncertainty&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;On the other hand &lt;em&gt;epistemic&lt;/em&gt; uncertainty is a kind of model-based uncertainty. Say we fit a model \(m\) to data, what we are really doing is choosing a model \(m\) from a set of models \(\mathcal{M}\). In most real-world cases we can never know if the model is the correct one, without an infeasibly large amount of data. So epistemic uncertainty is the uncertainty we have in our choice of model because we just didn’t collect enough data. For a continuum of models, we can again use variance as a proxy for epistemic uncertainty.&lt;/p&gt;

&lt;p&gt;Aleatoric uncertainty is important because it tells us how consistent our data is and epistemic uncertainty is useful because it tells us whether we can have confidence in our model or need more data. This should help us resolve the age0old question, “Why is my model so crap? Is it the model or the data”? The thing is, I think aleatoric and epistemic uncertainty are actually modelling the same thing…(sort of)!&lt;/p&gt;

&lt;h1 id=&quot;two-sides-of-the-same-coin&quot;&gt;Two sides of the same coin?&lt;/h1&gt;

&lt;p&gt;Let’s use an example to illustrate these two ideas.&lt;/p&gt;

&lt;p&gt;Consider a generative process. We&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;aleatoric is model mismatch&lt;/li&gt;
  &lt;li&gt;epistemic is lack of data&lt;/li&gt;
  &lt;li&gt;We have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;invented&lt;/code&gt; randomness because we are too lazy to model reality&lt;/li&gt;
  &lt;li&gt;bias-variance tradeoffs&lt;/li&gt;
  &lt;li&gt;Bayes’&lt;/li&gt;
  &lt;li&gt;discrete and continuous linear models? decompositions?
-no definitions&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Daniel Worrall</name><email>dworrall at qti.qualcomm.com</email></author><category term="probability" /><category term="bayesianism" /><category term="Jaynes" /><category term="philosophy" /><summary type="html">*This is a follow-on post from “On the ‘invention’ of randomness”**</summary></entry><entry><title type="html">Aleatoric and epistemic uncertainties: are they broken?</title><link href="http://localhost:4000/randomness4/" rel="alternate" type="text/html" title="Aleatoric and epistemic uncertainties: are they broken?" /><published>2019-12-29T01:00:00+01:00</published><updated>2019-12-29T01:00:00+01:00</updated><id>http://localhost:4000/randomness4</id><content type="html" xml:base="http://localhost:4000/randomness4/">&lt;p&gt;Most posts serve to educate by showing how something works, but this one does rather that opposite. It poses a question, one that I have been thinking about for some time. It’s about an inconsistent framework for describing aleatoric and epistemic uncertainties.&lt;/p&gt;

&lt;h4 id=&quot;1-the-variance-route&quot;&gt;1 The variance route&lt;/h4&gt;

&lt;p&gt;Let’s begin with a predictive model \(p(y \mid x, \theta)\), with input \(x\), real output \(y \in \mathbb{R}\), and parameters \(\theta\). Now say that by some process we have trained the model and have a posterior distribution \(p(\theta \mid \mathcal{D})\) in the parameters given the data. Then we can write the posterior predictive distribution as&lt;/p&gt;

\[p(y \mid x) = \int p(y \mid x, \theta) p(\theta \mid \mathcal{D}) \, \mathrm{d} \theta.\]

&lt;p&gt;By the &lt;a href=&quot;https://en.wikipedia.org/wiki/Law_of_total_variance&quot;&gt;law of total variance&lt;/a&gt;, we can decompose the variance of the posterior predictive into the following terms&lt;/p&gt;

\[\underbrace{\mathbb{V}_{p(y \mid x)}[y]}_{\text{total variance}} = \underbrace{\color{ForestGreen}{\mathbb{E}_{p(\theta \mid \mathcal{D})}[\mathbb{V}_{p(y \mid x, \theta)}[y]]}}_{\text{(1) aleatoric term}} + \underbrace{\color{MidnightBlue}{\mathbb{V}_{p(\theta \mid \mathcal{D})}[\mathbb{E}_{p(y \mid x, \theta)}[y]]}}_{\text{(2) epistemic term}}\]

&lt;p&gt;We see that the total variance is the sum of the average predictive variance and the variance of the predictive means. Term (1) is typically said to represent aleatoric uncertainty, being an average over variances it converges to a non-zero value with infinite data. The other term (2) measures fluctuations in the mean prediction across different parameters settings. This goes to zero with infinite data, so is typically associated with epistemic uncertainty. This decomposition is used in papers such as &lt;a href=&quot;https://arxiv.org/abs/1703.04977&quot;&gt;What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?&lt;/a&gt; (Kendall and Gal, 2017) and &lt;a href=&quot;https://arxiv.org/abs/1705.00664&quot;&gt;Bayesian Image Quality Transfer with CNNs: Exploring Uncertainty in dMRI Super-Resolution&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;2-the-entropic-route&quot;&gt;2 The entropic route&lt;/h4&gt;
&lt;p&gt;In the discrete setting people tend to use a different decomposition, an entropy decomposition. They write the following&lt;/p&gt;

\[\underbrace{H[Y | X=x]}_{\text{conditional entropy}} = \underbrace{\color{ForestGreen}{H[Y | \Theta, X=x]}}_{\text{(1) aleatoric term}} + \underbrace{\color{MidnightBlue}{\mathbb{I}[Y ; \Theta | X=x]}}_{\text{(2) epistemic term}}.\]

&lt;p&gt;Here we have paired up the conditional entropy with the total variance. Why does this make sense as a candidate for an aleatoric–epistemic decomposition? Well the aleatoric term (1) shows the residual uncertainty in \(Y\) given the learned distribution in \(\Theta\); that is, the variation in \(Y\) not captured by the model, which is very intuitive. The epistemic term (2) is a little more cryptic. It describes the reduction in uncertainty over the model because of the targets.&lt;/p&gt;

&lt;p&gt;The aleatoric term 1) is a mutual information between the parameters and the predictions&lt;/p&gt;</content><author><name>Daniel Worrall</name><email>dworrall at qti.qualcomm.com</email></author><category term="probability" /><category term="bayesianism" /><category term="Jaynes" /><category term="philosophy" /><summary type="html">Most posts serve to educate by showing how something works, but this one does rather that opposite. It poses a question, one that I have been thinking about for some time. It’s about an inconsistent framework for describing aleatoric and epistemic uncertainties.</summary></entry><entry><title type="html">On the ‘invention’ of randomness</title><link href="http://localhost:4000/blog/2019/12/the-invention-of-randomness/" rel="alternate" type="text/html" title="On the ‘invention’ of randomness" /><published>2019-12-15T00:00:00+01:00</published><updated>2019-12-15T00:00:00+01:00</updated><id>http://localhost:4000/blog/2019/12/randomness</id><content type="html" xml:base="http://localhost:4000/blog/2019/12/the-invention-of-randomness/">&lt;p&gt;&lt;img src=&quot;/media/jaynes-himself.jpg&quot; alt=&quot;The legend himself&quot; height=&quot;25%&quot; width=&quot;25%&quot; style=&quot;float: right;margin-left: 20px;margin-top: 7px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Recently in AMLAB we started a Jaynes reading group. &lt;a href=&quot;https://en.wikipedia.org/wiki/Edwin_Thompson_Jaynes&quot;&gt;E T Jaynes’&lt;/a&gt; posthumous book and general all-round cult classic &lt;em&gt;Probability Theory: The Logic of Science&lt;/em&gt; is the focus of our study. After having lectured a Bayesian statistics course for the last two years, I felt fairly confident in my understanding of the subject matter. It seems that while I am at ease with performing computations I have far from grasped Bayesianism from a conceptual, and some might say doctrinal, standpoint. And after a couple of conversations with others in a similar situation to me, it seems I may not be alone.&lt;/p&gt;

&lt;p&gt;Now this book is littered with gems and Jaynes’ colourful written-style is literary gold, but I want to focus on a small snippet, which got me thinking hard about my understanding of what it means to be &lt;em&gt;random&lt;/em&gt;. &lt;strong&gt;Jaynes essentially claims that randomness simply does not exist&lt;/strong&gt;. It is a human invention, which you can find in Digression 3.8.1. Let’s step through the logic.&lt;/p&gt;

&lt;h1 id=&quot;my-truth-versus-your-truth&quot;&gt;My truth versus your truth&lt;/h1&gt;
&lt;p&gt;In true statistical tradition, we are going to consider coin tosses. Let’s assume that randomness does exist. What do I mean by randomness? I mean that when I throw the coin in the air it will land heads or tails in a 100% unpredictable fashion. Some intrinsically indeterminate process will drive the coin to come to rest in a state, independent of when it left my fingers. In this unpredictable world an observer would be unable to make any sure judgements about the outcome. She may assume the prior probability of the coin landing heads or tails is 50%. This is not some weird quantum-y line of reasoning, the coin will land either heads or either tails, but we just cannot say beforehand. Some would argue, that maintaining a uniform probability distribution over the outcome of the coin toss is really the best she can do. It is a reflection of the truth of the world she lives in.&lt;/p&gt;

&lt;p&gt;Let’s now pretend that the world is 100% deterministic. Say I flip a coin and I happen to know its mass, moment of angular inertia, air resistance, initial momentum, etc. In this perfect world, an observer gifted with this knowledge would be able to predict with 100% accuracy whether it lands heads or tails. Even if the computations are intractable and we are reduced to brute-forcing over possible futures, we could at least agree that the outcome is calculable in principle. No randomness here. Now let’s add a twist. Let’s say that we do not tell the observer any of this privileged information about the physics of the coin-toss. In that case, the observer would be reduced to educated guesses about the outcome of the toss. Because she’s a good Bayesian, she will continue to assume the prior probability of the coin landing heads or tails is 50%, but now her motivations are different. Or are they?&lt;/p&gt;

&lt;p&gt;Maintaining a probability distribution over the outcome of the coin toss is not just a cynically non-committal statement. It is a full and honest acknowledgement of her ignorance. She has not claimed that the world itself is random in any way—in fact she very well knows it’s deterministic—but she is merely asserting that her knowledge of its physical parameters are unknown to her. Whichever way the coin lands, the outcome will be a surprise to her, because she is unable to predict it with her imperfect knowledge.&lt;/p&gt;

&lt;p&gt;This seemed a bit weird to me. In moving from the synthetically random world to the deterministic one, it would seem that modelled randomness is really just a statement of observer ignorance, rather than any innate property of the universe around us. The fascinating point for me is that from the point-of-view of the observer, &lt;em&gt;it does not matter whichever world she lives in (the inherently random or the inherently deterministic), in both cases she is forced to use the same mathematical reasoning!&lt;/em&gt; To make a linguistic distinction between the intrinsic randomness of the outside world and the observer’s perceived randomness, we will term them &lt;em&gt;intrinsic randomness&lt;/em&gt; and &lt;em&gt;epistemic randomness&lt;/em&gt;. Epistemic randomness is the uncertainty I have over the outside world because I simply do not have enough information. It seems that epistemic randomness is uncontroversial. Intrinsic randomness on the other hand is this very much controversial concept that Nature itself is indeterminate and unpredictable.&lt;/p&gt;

&lt;p&gt;Now this kind of thinking leads to a vast array of new questions. Could an observer ever determine whether the world she lives in is intrinsically random or just deterministic, given that her tools to analyse both are the same? If intrinsic randomness does not exist, is it then some kind of invention? What does this mean for the interpretation of stochastically derived quantities such as aleatoric and epistemic uncertainty? What about free will? (This last question is a little cliché, but people seem to like talking about free will). &lt;strong&gt;I plan to devote a follow up blog to aleatoric and epistemic uncertainties.&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&quot;my-gripe-with-jaynes&quot;&gt;My gripe with Jaynes&lt;/h1&gt;
&lt;p&gt;Those acquainted with Jaynes’ writings will be all too familiar with his grandiloquent rhetorical style. He really seems to believe that intrinsic randomness does not exist, that it is a sort of what he calls &lt;em&gt;mind projection fallacy&lt;/em&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The belief that ‘randomness’ is some kind of real property existing in Nature is a form of the mind projection fallacy which says, in effect, ‘I don’t know the detailed causes – therefore – Nature does not know them.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Of course &lt;em&gt;he&lt;/em&gt; would put (intrinsic) &lt;em&gt;randomness&lt;/em&gt; in quotation marks. Mind projection fallacies, a very Jaynesian invention, are assertions wherein an observer states that how they see the world really is reality. My reality is your reality and everyone else’s too. That someone should disagree on the nature of Nature itself is simple idiocy. Now my gripe with Jaynes’ stance is that he himself appears to be stuck in a mind projection fallacy of his own. His assertion that intrinsic randomness does not exist is a projection of his reality on to the reader. Just for fun here is another, and I daresay somewhat salacious, Jaynes quote&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;For some, declaring a problem to be ‘randomized’ is an incantation with the same purpose and effect as those uttered by an exorcist to drive out evil spirits; i.e. it cleanses their subsequent calculations and renders them immune to criticism. We agnostics often envy the True Believer, who thus acquires so easily that sense of security which is forever denied to us.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;quantum-weirdness&quot;&gt;Quantum weirdness&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Disclaimer: in this next bit I talk about physics, but be under no illusions, I am far from knowledgeable on this subject.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;So is the world deterministic? As far as I know the main source of randomness in the world rises out of the depths from the sub-atomic quantum world. The quantum world is very strange in that very small objects, called particles, such as electrons and neutrons, are described probabilistically and not using our everyday Newtonian world-view. Particles are seen to exhibit &lt;em&gt;wave-particle duality&lt;/em&gt;—a behaviour where they act like waves, being able to be in multiple locations at once—until they are observed, at which point we observe a bizarre effect called &lt;em&gt;wavefunction collapse&lt;/em&gt;, where they then assume a definite location in space. Wavefunction collapse has always been an incomprehensible phenomenon to me. Why should the act of observation change the nature of the underlying physics?&lt;/p&gt;

&lt;p&gt;The central apparatus for modelling particles is a &lt;em&gt;wavefunction&lt;/em&gt;, a function extending over all space and time. The squared modulus of the wavefunction is equal to the probability that a particle will be observed at a specific location and time if a measurement is taken. One of the big problems in quantum mechanics is understanding how to interpret the wavefunction. Is it a fundamental physical object? If so, it is very strange indeed, since in the classical setting where we model big objects, we never observe an object to be in two places at once. In the quantum world, this happens by necessity.&lt;/p&gt;

&lt;p&gt;One school of thinking, in fact the one I learnt in secondary school (that’s high school for everyone else), is that the wavefunction is a fundamental object of nature. It is part of reality. This is an unsettling idea, but if true it would indicate that our world is very weird indeed. Einstein famously rejected this interpretation claiming “God does not play dice”. This is the (in)famous &lt;em&gt;Copenhagen interpretation&lt;/em&gt; of quantum mechanics, which as it turns out is not universally accepted or rejected by all physicists. Some detractors indeed do subscribe to deterministic &lt;em&gt;hidden variable theories&lt;/em&gt;, as I believe did Jaynes, in which random quantum effects are just disturbances caused by so-called hidden variables, unobserved by us humans (so far). But I think that one flavour of hidden variable theories, called local hidden variable theories, have been ruled out already by what is known as the Bell test experiments. A much more promising route is &lt;em&gt;QBism&lt;/em&gt;, pronounced &lt;em&gt;cubism&lt;/em&gt; like the artistic movement, championed by Christopher Fuchs. QBism was originally called Quantum Bayesianism, but apparently many hardcore Bayesians have pointed out it is not strictly Bayesian and QBism sounds cooler anyway. In this theory particles occupy one position at any one time, the wavefunction is just an expression of observer ignorance, and wavefunction collapse is analogous to making a measurement and updating our beliefs. Mystery resolved…if it’s true.&lt;/p&gt;

&lt;p&gt;So whether intrinsic randomness does exists is an open and potentially unanswerable question. At least to determine the potentiality of intrinsic randomness would require us to step outside of the rôle of observer—a purely unscientific practice. All observers are bound to make world inferences via the methods of epistemic randomness and are thus at risk of succumbing to Jaynesian mind projection fallacies. Some might go as far to say that if something is unverifiable by experiment it cannot &lt;em&gt;exist&lt;/em&gt;. Enter die-hard scientific methodists (theological pun intended). On a side note, the question of existence seems to be an argument I get into a lot nowadays with post-structuralists from the arts and humanities, who always tell me that scientists are mistaken in believing that what they study exists.&lt;/p&gt;

&lt;h2 id=&quot;inevitably-after-reading-a-story-about-flipping-coins-i-seem-to-have-found-myself-in-quite-the-bind-if-i-have-come-to-understand-anything-it-is-that-you-can-never-be-safe-in-thinking-you-know-anything&quot;&gt;Inevitably after reading a story about flipping coins, I seem to have found myself in quite the bind. If I have come to understand anything, it is that you can never be safe in thinking you know anything.&lt;/h2&gt;</content><author><name>Daniel Worrall</name><email>dworrall at qti.qualcomm.com</email></author><category term="probability" /><category term="bayesianism" /><category term="Jaynes" /><category term="philosophy" /><summary type="html">Recently in AMLAB we started a Jaynes reading group. E T Jaynes' posthumous book and general all-round cult classic Probability Theory: The Logic of Science is the focus of our study. After having lectured a Bayesian statistics course for the last two years...</summary></entry><entry><title type="html">Symmetry 2: A Brief Sketch</title><link href="http://localhost:4000/symmetry2/" rel="alternate" type="text/html" title="Symmetry 2: A Brief Sketch" /><published>2019-07-07T02:00:00+02:00</published><updated>2019-07-07T02:00:00+02:00</updated><id>http://localhost:4000/symmetry2</id><content type="html" xml:base="http://localhost:4000/symmetry2/">&lt;h1 id=&quot;the-universe-is-symmetrical&quot;&gt;The universe is symmetrical&lt;/h1&gt;
&lt;p&gt;In the last few years, there has been a lot of interest in an area called &lt;em&gt;equivariance&lt;/em&gt;. Synonyms include, &lt;em&gt;symmetry&lt;/em&gt;, &lt;em&gt;invariance&lt;/em&gt;, and &lt;em&gt;covariance&lt;/em&gt; (different to the covariance we see in statistics) and I will probably use them interchangeably. In this and some planned posts, I hope to demystify what equivariance is, why you should be excited about it, and what this implies for the field of machine learning. Before we can understand equivariance, however, we are going to take a detour into physics.&lt;/p&gt;

&lt;p&gt;Anyone familiar with modern physics will know that it is written entirely from the point of view of invariances. Let’s consider a high school example. If I hurl a tennis ball into the air, Newton’s laws of motion will pull it back down to Earth. So much is true whether I hurled it in London or in Amsterdam. It also does not matter whether I’m facing due North or into the Sun—admittedly there is not much Sun in either London or Amsterdam. Furthermore, I could stand on top of a train, Great Escape style, and neglecting wind resistance, those very same familiar laws of physics apply. Physicists and mathematicians say that the laws of physics are &lt;em&gt;invariant&lt;/em&gt; to position, orientation, and velocity (as long as you’re not travelling close to the speed of light).&lt;/p&gt;

&lt;p&gt;The fact that the laws of physics do not depend on where you are is a fundament of the field, and &lt;em&gt;it allows us to make predictions&lt;/em&gt;. If the physics in Kolkatta were different to the physics in Chicago, imagine the vast amounts of effort required to figure them out, (not to mention the number of papers we would publish as a result). That we only need to figure out one set of rules that apply everywhere not only saves time and effort, but in some sense, it provides a minimal description of the universe. If I am to make a tentative machine learning analogy, the existence of invariances allows scientists to &lt;em&gt;generalise&lt;/em&gt;. Beyond position, orientation, and velocity, physics actually hosts a whole league of invariances from the exotic \(SU(3)\) symmetries of quantum chromodynamics to the gauge symmetries of electromagnetism.&lt;/p&gt;

&lt;p&gt;“Now this is all very nice and fancy, but how does it help me progress in my noble endeavour to build the world’s most accurate dog detector”, you say. In essence, the aim of machine learning scientists in the field of equivariance is to adopt the 100+ year long trend in physics and related fields, and implement symmetry principles in our very own corner of science. The promised payoffs are better generalisation (which naturally implies better data efficiency), tighter control of what our models can learn, and some beautiful maths.&lt;/p&gt;

&lt;h1 id=&quot;symmetry-in-machine-learning&quot;&gt;Symmetry in machine learning&lt;/h1&gt;
&lt;p&gt;So let’s consider some examples of symmetry in machine learning. A common example I like to give is from classification. In the dog picture below, we see two dogs, indeed, the exact same dog side-by-side. Both images contain the exact same semantic content, but at a horizontal translation, indicated by the arrows. Now to a human, this is trivial and what I have just told you has probably not revolutionised your world, but to a computer this is not the case. It just sees two different images. What a computer’s visual system lacks is a mechanism to say that these two images are of the same concept, but transformed versions of each other.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/translated_doggie.png&quot; alt=&quot;A cute pic of a dog&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;what-is-symmetry&quot;&gt;What is symmetry?&lt;/h1&gt;
&lt;p&gt;Let’s lay down this idea with a little mathematics. We are going to denote a function \(f: \mathcal{X} \to \mathcal{Y}\) from image space \(\mathcal{X}\) to some output space \(\mathcal{Y}\). This function could represent a single neural network layer, or an entire neural net. And the output space \(\mathcal{Y}\) could be a label space, another image space, an activation space or something else entirely. We are also going to introduce something called a &lt;em&gt;transformation operator&lt;/em&gt; \(\mathcal{T}_\theta: \mathcal{X} \to \mathcal{X}\). This is also a very abstract concept. If \(\mathcal{X}\) is image space, then \(T_\theta\) could represent a rotation for instance.&lt;/p&gt;

&lt;p&gt;We say that \(f\) is &lt;em&gt;invariant&lt;/em&gt; to \(T_\theta\) if for all \(\theta\)&lt;/p&gt;

\[f(x) = f(T_{\theta}[x]).\]

&lt;p&gt;The set of all \(T_\theta\) such that this property holds is called the &lt;em&gt;symmetries&lt;/em&gt; of \(f\). Now we usually assume that \(f\) may represent some groundtruth function, out there in the universe, which we are trying to discover. It is the ultimate dog classification function, or the ultimate French to Japanese mapping. If this is the case, then \(f\) really represents a &lt;em&gt;task&lt;/em&gt;, and so &lt;strong&gt;symmetry is a property of the task, not the data&lt;/strong&gt;. This is a common misconception many stumble into. The field of equivariance is dedicated to building expressive functions \(f\), with built-in symmetries \(\{T_\theta\}_\theta\).&lt;/p&gt;

&lt;p&gt;In the next post, we’ll dive into to maths behind equivariance and start to really build up one of the languages in which we can think about symmetry in machine learning.&lt;/p&gt;</content><author><name>Daniel Worrall</name><email>dworrall at qti.qualcomm.com</email></author><category term="equivariance" /><category term="deep-learning" /><category term="convolution" /><summary type="html">The universe is symmetrical In the last few years, there has been a lot of interest in an area called equivariance. Synonyms include, symmetry, invariance, and covariance (different to the covariance we see in statistics) and I will probably use them interchangeably. In this and some planned posts, I hope to demystify what equivariance is, why you should be excited about it, and what this implies for the field of machine learning. Before we can understand equivariance, however, we are going to take a detour into physics.</summary></entry><entry><title type="html">Symmetry: A Brief Sketch</title><link href="http://localhost:4000/symmetry1/" rel="alternate" type="text/html" title="Symmetry: A Brief Sketch" /><published>2019-05-28T02:00:00+02:00</published><updated>2019-05-28T02:00:00+02:00</updated><id>http://localhost:4000/symmetry1</id><content type="html" xml:base="http://localhost:4000/symmetry1/">&lt;h1 id=&quot;the-universe-is-symmetrical&quot;&gt;The universe is symmetrical&lt;/h1&gt;
&lt;p&gt;In the last few years, there has been a lot of interest in an area called &lt;em&gt;equivariance&lt;/em&gt;. Synonyms include, &lt;em&gt;symmetry&lt;/em&gt;, &lt;em&gt;invariance&lt;/em&gt;, and &lt;em&gt;covariance&lt;/em&gt; (different to the covariance we see in statistics) and I will probably use them interchangeably. In this and some planned posts, I hope to demystify what equivariance is, why you should be excited about it, and what this implies for the field of machine learning. Before we can understand equivariance, however, we are going to take a detour into physics.&lt;/p&gt;

&lt;p&gt;Anyone familiar with modern physics will know that it is written entirely from the point of view of invariances. Let’s consider a high school example. If I hurl a tennis ball into the air, Newton’s laws of motion will pull it back down to Earth. So much is true whether I hurled it in London or in Amsterdam. It also does not matter whether I’m facing due North or into the Sun—admittedly there is not much Sun in either London or Amsterdam. Furthermore, I could stand on top of a train, Great Escape style, and neglecting wind resistance, those very same familiar laws of physics apply. Physicists and mathematicians say that the laws of physics are &lt;em&gt;invariant&lt;/em&gt; to position, orientation, and velocity (as long as you’re not travelling close to the speed of light).&lt;/p&gt;

&lt;p&gt;The fact that the laws of physics do not depend on where you are is a fundament of the field, and &lt;em&gt;it allows us to make predictions&lt;/em&gt;. If the physics in Kolkatta were different to the physics in Chicago, imagine the vast amounts of effort required to figure them out, (not to mention the number of papers we would publish as a result). That we only need to figure out one set of rules that apply everywhere not only saves time and effort, but in some sense, it provides a minimal description of the universe. If I am to make a tentative machine learning analogy, the existence of invariances allows scientists to &lt;em&gt;generalise&lt;/em&gt;. Beyond position, orientation, and velocity, physics actually hosts a whole league of invariances from the exotic \(SU(3)\) symmetries of quantum chromodynamics to the gauge symmetries of electromagnetism.&lt;/p&gt;

&lt;p&gt;“Now this is all very nice and fancy, but how does it help me progress in my noble endeavour to build the world’s most accurate dog detector”, you say. In essence, the aim of machine learning scientists in the field of equivariance is to adopt the 100+ year long trend in physics and related fields, and implement symmetry principles in our very own corner of science. The promised payoffs are better generalisation (which naturally implies better data efficiency), tighter control of what our models can learn, and some beautiful maths.&lt;/p&gt;

&lt;h1 id=&quot;symmetry-in-machine-learning&quot;&gt;Symmetry in machine learning&lt;/h1&gt;
&lt;p&gt;So let’s consider some examples of symmetry in machine learning. A common example I like to give is from classification. In the dog picture below, we see two dogs, indeed, the exact same dog side-by-side. Both images contain the exact same semantic content, but at a horizontal translation, indicated by the arrows. Now to a human, this is trivial and what I have just told you has probably not revolutionised your world, but to a computer this is not the case. It just sees two different images. What a computer’s visual system lacks is a mechanism to say that these two images are of the same concept, but transformed versions of each other.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/media/translated_doggie.png&quot; alt=&quot;A cute pic of a dog&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;what-is-symmetry&quot;&gt;What is symmetry?&lt;/h1&gt;
&lt;p&gt;Let’s lay down this idea with a little mathematics. We are going to denote a function \(f: \mathcal{X} \to \mathcal{Y}\) from image space \(\mathcal{X}\) to some output space \(\mathcal{Y}\). This function could represent a single neural network layer, or an entire neural net. And the output space \(\mathcal{Y}\) could be a label space, another image space, an activation space or something else entirely. We are also going to introduce something called a &lt;em&gt;transformation operator&lt;/em&gt; \(\mathcal{T}_\theta: \mathcal{X} \to \mathcal{X}\). This is also a very abstract concept. If \(\mathcal{X}\) is image space, then \(T_\theta\) could represent a rotation for instance.&lt;/p&gt;

&lt;p&gt;We say that \(f\) is &lt;em&gt;invariant&lt;/em&gt; to \(T_\theta\) if for all \(\theta\)&lt;/p&gt;

\[f(x) = f(T_{\theta}[x]).\]

&lt;p&gt;The set of all \(T_\theta\) such that this property holds is called the &lt;em&gt;symmetries&lt;/em&gt; of \(f\). Now we usually assume that \(f\) may represent some groundtruth function, out there in the universe, which we are trying to discover. It is the ultimate dog classification function, or the ultimate French to Japanese mapping. If this is the case, then \(f\) really represents a &lt;em&gt;task&lt;/em&gt;, and so &lt;strong&gt;symmetry is a property of the task, not the data&lt;/strong&gt;. This is a common misconception many stumble into. The field of equivariance is dedicated to building expressive functions \(f\), with built-in symmetries \(\{T_\theta\}_\theta\).&lt;/p&gt;

&lt;p&gt;In the next post, we’ll dive into to maths behind equivariance and start to really build up one of the languages in which we can think about symmetry in machine learning.&lt;/p&gt;</content><author><name>Daniel Worrall</name><email>dworrall at qti.qualcomm.com</email></author><category term="equivariance" /><category term="deep-learning" /><category term="convolution" /><summary type="html">The universe is symmetrical In the last few years, there has been a lot of interest in an area called equivariance. Synonyms include, symmetry, invariance, and covariance (different to the covariance we see in statistics) and I will probably use them interchangeably. In this and some planned posts, I hope to demystify what equivariance is, why you should be excited about it, and what this implies for the field of machine learning. Before we can understand equivariance, however, we are going to take a detour into physics.</summary></entry></feed>